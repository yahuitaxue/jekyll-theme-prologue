---
title: 服务架构
author: Yahui
layout: php
category: PHP
---


书名：《-》

<pre style="text-align: left;">
	1.nginx使用upstream模块配置负载均衡(名字叫swoft_nginx)(在location中配置反向代理到负载均衡的 swoft_nginx)
	2.配置服务发现
		nginx配置文件upstream模块使用upsync加载consul
		upstream swoole_consul {
			server 192.168.169.140:9001; #留一个固定服务否则nginx启动报错
			upsync 127.0.0.1:8500/v1/kv/upstreams/swoole_test upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=on;
			upsync_dump_path /redis_2004/17/conf/servers_test.conf; #从consul读取的信息生成配置文件
			include /redis_2004/17/conf/servers_test.conf; #引入这个配置文件
		}
		解释：
			127.0.0.1:8500/v1/kv/upstreams -> 连接consul的api资源地址
			swoole_test -> 相当于我们自己在consul中自定义的key
			upsync_timeout -> 超时时间6分钟
			upsync_interval -> 定时获取信息的时间
			upsync_type -> 类型
			strong_dependency=on; -> 是否依赖consul运行
			upsync_dump_path -> 拉取之后申请配置文件
	3.在nginx中配置好consul后,向consul增加swoft的集群地址,这样nginx就可以动态加载swoft
	总结:
		nginx使用upsync动态获取consul中的服务信息, strong_dependency是否依赖, 获取服务信息后, 生成一份配置文件(比如swoft_server.conf)
		1.构建3swoft,1nginx,1consul
		2.配置nginx upstream 异步访问consul服务端
		3.consul:实际在程序自动注册信息到consul (当时使用的是curl)
	3.docker-compose配置(部分代码)
		# 编排php,redis,nginx容器
		version: "3.6" # 确定docker-composer文件的版本
		services: # 代表就是一组服务 - 简单来说一组容器
		  # server
		  consul_master_server_170_30: # 这个表示服务的名称，课自定义; 注意不是容器名称
		    image: consul1.4 # 指定容器的镜像文件
		    ports: # 配置容器与宿主机的端口
		      - "8500:8500"
		    networks: # 引入外部预先定义的网段
		       consul:
		         ipv4_address: 170.200.7.30   #设置ip地址
		    container_name: consul_master_server_170_30 # 这是容器的名称
		    volumes: #配置数据挂载
		      - 挂载目录(测试可以指定同一目录(因为在同一服务器上, 集群环境可以使用git+Jenkins), 实现代码同步)
		    command: ./consul agent -server -bootstrap-expect 3 -data-dir /tmp/consul -node=consul_master_server_170_30 -bind=170.200.7.30 -ui -client=0.0.0.0
		    depends_on: # 容器启动依赖的顺序
		    	- mongod_shard_A_174_2
		    注：
		    	-server // 代表是一个服务
		    	-bootstrap-expect 3 // 只在master节点，表示启动多少个节点（可以没有）
		    	-data-dir /tmp/consul
		    	-node=consul_master_server_170_30 // 节点名称
		    	-bind=170.200.7.30 // 绑定网络的通信方式，有IP或者端口映射
		    	-ui // consul的web界面
		    	-client=0.0.0.0
		    	-join=170.200.7.30  // 绑定到哪个节点，从节点（slave）会有
	4.swoft
		可以利用注册与停止时间增加服务发现事件(官方提供的有方法(注:参数是服务的名称)),来实swoft的注册与删除
	5.docker-compose配置多个服务的问题
		问题:代码同步指定同一目录(swoft), 而swoft的配置文件是同一个,导致服务发现只能发现一个
		处理:使用服务器启动时的ip地址来配置.env文件,swoft启动读取.env文件来配置,从而实现同一套代码配置文件是多套
	6.在swoft的listener下的RegisterServiceListener中,配置check模式
		"check" => [
			"name" => "swoft.goods.server" // 这个主要是区分与其他
			"tcp" => "你的swoft宿主机地址" // 这个如果是http，可以写成http的地址(http://test.com)
			"interval" => "10s" // 如果及时性比较低,可以设置长一些,这样对服务器压力小一点
			"timeout" => "2s" // 设置超时时间
		] 这个是用到swoft-consul组件, 这样consul就会生成一个定时器,检测swoft的健康状况
		总之,利用docker-composer,增加启动命令,给shell脚本传递IP,端口,shell脚本处理传递过来的参数,进行拼接,最终写进.env文件中,(形如HOST=12.12.12.11....),swoft启动读取.env文件内容获取IP及端口(例如第6点,将swoft地址拼接到swoft-consul组件配置项中),最终达到consul定时检测通过docker-composer启动的swoft服务
	7.nginx使用lua模块（以lua使用redis为例）
		1.nginx 通过 --add-module 加载nginx-lua模块(如果是通过yum安装， 需要通过nginx -V查看版本，然后再编译安装相同版本，编译安装的时候加载模块)
		2.nginx配置文件中，加载文件（lua_package_path "****/redis.lua;;"） #在server外层
		3.nginx配置文件中，使用content_by_lua_file *****/lua_redis; #这个文件是lua操作redis的代码，在server中,或者location中
		4.获取参数
			local request_method ngx.var.request_method;
			if "GET" == request_method then
				args = ngx.req.get_uri_args();
			elseif "POST" == request_method then
				args = ngx.req.get_post_args();
			end
			params = args["sku_id"];
	8.主从同步过程
		<span class="image featured"><img src="{{ 'assets/images/other/mysqlbinlog.jpg' | relative_url }}" alt="" /></span>
		binlog是二进制日志文件，可使用mysql自带mysqlbinlog工具查看 mysqlbinlog myql-bin.000001
		注(binlog)：
			1.binlog文件会随服务的启动创建一个新文件
			2.通过flush logs 可以手动刷新日志，生成一个新的binlog文件
			3.通过show master status 可以查看binlog的状态
			4.通过reset master 可以清空binlog日志文件
			5.通过mysqlbinlog 工具可以查看binlog日志的内容
			6.通过执行dml，mysql会自动记录binlog
		主从搭载过程
			1.配置三台服务器并安装MySQL
			2.主库开启binlog日志，查看状态 show master status/G;
			3.在从节点配置可配置MySQL配置文件，也可执行下面的语句
				change master to
				master_host='192.168.232.101',
				master_user='repl',
				master_password='123456',
				master_log_file='mysql- bin.000001',
				master_log_pos=154;
			4.开启从节点start slave;
	9.服务器之间加密通信
		每个节点都执行
		[root@100 ~]# cd ~
		[root@100 ~]# ssh-keygen -t rsa #看到提示不用管，一路回车就是
		[root@100 ~]# cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
		[root@100 ~]# chmod 600 ~/.ssh/authorized_keys
		只要在一个节点执行即可。这里在 192.168.232.100上执行
		[root@100 ~]# ssh 192.168.232.101 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
		[root@100 ~]# ssh 192.168.232.102 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
		[root@100 ~]# ssh 192.168.232.103 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
		分发整合后的文件到其它节点
		[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.101:~/.ssh/
		[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.102:~/.ssh/
		[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.103:~/.ssh/
	10.es集群搭建
		服务器配置，三台centos虚拟机，ip列表如下：
			192.168.52.131
			192.168.52.132
			192.168.52.133
		安装es之前先安装jdk，jdk的安装略去。
		es的版本：elasticsearch-6.5.4

		三台服务器es安装路径信息

		[root@master app]# pwd
		/usr/local/app
		[root@master app]# ls
		elasticsearch-6.5.4  elasticsearch-6.5.4.zip  jdk1.8.0_191
		三台服务器配置如下：

		192.168.52.131配置信息：

		[root@master elasticsearch-6.5.4]# vim config/elasticsearch.yml
		#配置es的集群名称，默认是elasticsearch，
		#es会自动发现在同一网段下的es，
		# 如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群。
		cluster.name: cell
		#
		# ------------------------------------ Node ------------------------------------
		node.name: node_01
		node.master: true
		node.data: true
		#
		# Add custom attributes to the node:
		#
		#node.attr.rack: r1
		#
		# ----------------------------------- Paths ------------------------------------
		#
		# Path to directory where to store the data (separate multiple locations by comma):
		#
		path.data: /var/data/elasticsearch
		#
		# Path to log files:
		#
		path.logs: /var/log/elasticsearch
		network.host: 0.0.0.0
		http.port: 9200
		transport.tcp.port: 9300

		discovery.zen.ping.unicast.hosts: ["192.168.52.131:9300","192.168.52.132:9300", "192.168.52.133:9300"]

		discovery.zen.minimum_master_nodes: 2 
		192.168.52.132配置信息：

		只需要修改node.name即可：

		node.name: node_02

		192.168.52.133配置信息：

		node.name: node_03

		分别启动三台服务器上的es，（这里不能使用root用户启动，请创建个普通用户，如何创建，略去）

		查看集群信息：

		浏览器访问：http://192.168.52.131:9200/_cat/nodes?v

		显示结果如下：

		ip             heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
		192.168.52.132           13          94   6    0.11    0.26     0.23 mdi       -      node_02
		192.168.52.133           14          93   4    0.10    0.17     0.14 mdi       *      node_03
		192.168.52.131           13          92  15    0.22    0.50     0.28 mdi       -      node_01
		到此搭建成功了。。
	11.安装go
		下载go包并安装
		配置环境变量
			vi /etc/profile, 在export后加bin路径
			重启环境变量 source /etc/profile
	12.降级(降级主要针对不是特别重要但是比较占用资源的模块)
		1.降级的种类
			根据开关的位置分为：代码降级和前置降级
			读写降级：读降级和写降级
				读降级：数据从MySQL->缓存(redis)->静态资源->兜底文件(就是默认)->nginx返回(直接返回空)
			(nginx的限流:ngx_http_limit_req_module,ngx_http_limit_conn_module模块)
		2.手动降级,自动降级
		操作：
			1.nginx加载lua模块
			2.lua文件从redis读取开关设置来判断是从哪里获取数据(微服务/缓存/静态文件/...)
		3.漏桶原理(基本都是nginx实现),还有多(nginx+lua实现)
			需要依赖lua-resty-limit-traffic模块(是openresty的一个扩展)
			
			-- 加载nginx—lua限流模块
			local limit_req = require "resty.limit.req"
			-- 因为模块中控制粒度为毫秒级别，所以可以做到毫秒级别的平滑处理
			local lim, err = limit_req.new("my_limit_req_store", 50, 1000) #这里只是一个计算值,这里设置rate=50个请求/每秒，漏桶桶容量设置为1000个请求
			if not lim then
			  ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
			  return ngx.exit(501)
			end
			local key = ngx.var.binary_remote_addr
			local delay, err = lim:incoming(key, true)
			ngx.say(delay)
			if ( delay <0 or delay==nil ) then
			  return ngx.exit(502)
			end
			-- delay值就是当前这个请求的等待时长，这个时长是通过resty.limit.req模块计算出来的
			-- 1000以外的就溢出
			if not delay then
			  if err == "rejected" then
			    return ngx.say("1000以外的就溢出")
			    -- return ngx.exit(502)
			  end
			  ngx.log(ngx.ERR, "failed to limit req: ", err)
			  return ngx.exit(502)
			-- 加载nginx—lua限流模块
			local limit_req = require "resty.limit.req"
			-- 这里设置rate=50个请求/每秒，漏桶桶容量设置为1000个请求
			-- 因为模块中控制粒度为毫秒级别，所以可以做到毫秒级别的平滑处理
			local lim, err = limit_req.new("my_limit_req_store", 50, 1000)
			if not lim then
			  ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
			  return ngx.exit(501)
			end
			local key = ngx.var.binary_remote_addr
			local delay, err = lim:incoming(key, true)
			ngx.say(delay)
			if ( delay <0 or delay==nil ) then
			  return ngx.exit(502)
			end
			-- delay值就是当前这个请求的等待时长，这个时长是通过resty.limit.req模块计算出来的
			-- 1000以外的就溢出
			if not delay then
			  if err == "rejected" then
			    return ngx.say("1000以外的就溢出")
			    -- return ngx.exit(502)
			  end
			  ngx.log(ngx.ERR, "failed to limit req: ", err)
			  return ngx.exit(502)
			end
			-- 50-100的等待从微服务+mysql获取实时数据;（100-50）/50 =1
			if ( delay >0 and delay <=1 ) then
			  ngx.sleep(delay)
			-- 100-400的直接从redis获取实时性略差的数据;（400-50）/50 =7
			elseif ( delay >1 and delay <=7 ) then
			  local resp, err = redis_instance:get("redis_goods_list_advert")
			  ngx.say(resp)
			  return
			-- 400-1000的从静态文件获取实时性非常低的数据（1000-50）/50 =19
			elseif ( delay >7) then
			  ngx.header.content_type="application/x-javascript;charset=utf-8"
			  local file = "/etc/nginx/html/goods_list_advert.json"
			  local f = io.open(file, "rb")
			  local content = f:read("*all")
			  f:close()
			  ngx.print(content)
			  return
			end
			ngx.say("进入查询微服务+mysql") #实际中,这一步就是正常请求微服务处理数据
		4.限制每个IP的请求速度(nginx中limit_req_zone),需要下载nginx官方限流模块
		limit_req_zone $binary_remote_addr zone=one:10m rate=60r/m;
			server {
					...
						location /login.html {
						limit_req zone=one;
					...
				}
			}
</pre>